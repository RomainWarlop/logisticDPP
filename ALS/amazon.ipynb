{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy \n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "sqlCtx = SQLContext(sc)\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = [\"diaper\",\"apparel\",\"feeding\"]\n",
    "\n",
    "data = {}\n",
    "nItemByCat = {}\n",
    "deltaByCat = {}\n",
    "nItem = 0\n",
    "for cat in category:\n",
    "    data[cat] = pd.read_csv(\"/home/romain/Documents/PhD/logisticDPP/amazon/1_100_100_100_\"+cat+\"_regs.csv\",\n",
    "                   sep=\";\",header=None,names=['itemSet'])\n",
    "    \n",
    "    data[cat]['setSize'] = list(map(lambda x: len(x.split(',')),data[cat]['itemSet']))\n",
    "    items = list(map(lambda x: list(map(int,x.split(','))),data[cat]['itemSet']))\n",
    "    items = list(set([item for sublist in items for item in sublist]))\n",
    "    nItemByCat[cat] = len(items)\n",
    "    deltaByCat[cat] = nItem\n",
    "    nItem += len(items)\n",
    "\n",
    "sets = []\n",
    "for cat in category:\n",
    "    tmp = list(map(lambda x: x.split(','),data[cat]['itemSet']))\n",
    "    tmp = list(map(lambda x: list(map(lambda y: int(y)+deltaByCat[cat],x)),tmp))\n",
    "    sets.extend(tmp)\n",
    "size = list(map(len,sets))\n",
    "nUsers = len(sets)\n",
    "users = np.repeat(range(nUsers),size)\n",
    "\n",
    "flatsets = [int(item)-1 for sublist in sets for item in sublist]\n",
    "df = pd.DataFrame({'user':users,'item':flatsets,'rating':1.0})\n",
    "df.index = df['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mask(x):\n",
    "    result = np.zeros_like(x)\n",
    "    if len(x)>1:\n",
    "        result[np.random.choice(len(x))] = 1\n",
    "    return result\n",
    "\n",
    "threshold   = 0.7\n",
    "numTraits   = 30 if len(category)==1 else 60\n",
    "nRuns       = 3\n",
    "MPR         = []\n",
    "Ks          = [5,10,20]\n",
    "P           = dict.fromkeys(Ks)\n",
    "for K in Ks:\n",
    "    P[K] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run number 1 - 60\n",
      "run number 2 - 60\n",
      "run number 3 - 60\n",
      "MPR= 89.50933772219582\n",
      "Precision @5= 15.777830650556796\n",
      "Precision @10= 25.371560136504986\n",
      "Precision @20= 42.154626395943225\n"
     ]
    }
   ],
   "source": [
    "for run in range(nRuns):\n",
    "    print(\"run number\",run+1,\"-\",numTraits)\n",
    "    np.random.seed(123*run)\n",
    "    \n",
    "    testUsers = list(np.random.choice(range(nUsers),size=int((1-threshold)*nUsers),replace=False))\n",
    "    trainingUsers = list(set(range(nUsers))-set(testUsers))\n",
    "    trainingData = df.loc[trainingUsers]\n",
    "    trainingData.index = range(len(trainingData))\n",
    "    \n",
    "    testData = df.loc[testUsers]\n",
    "    testData.index = range(len(testData))\n",
    "    \n",
    "    mask = testData.groupby(['user'])['user'].transform(random_mask).astype(bool)\n",
    "    not_mask = list(map(lambda x: not(x),mask))\n",
    "    \n",
    "    trainingData = pd.concat([trainingData,testData.loc[not_mask]])\n",
    "    testData = testData.loc[mask]\n",
    "    \n",
    "    sparkInput = sqlCtx.createDataFrame(trainingData)\n",
    "    als = ALS(rank=numTraits,regParam=0.1,userCol='user',itemCol='item',ratingCol='rating')\n",
    "    mod = als.fit(sparkInput) \n",
    "    \n",
    "    testUsers = list(set(testData['user']))\n",
    "    testUsers = pd.DataFrame({'user':testUsers})\n",
    "    testUsers = sqlCtx.createDataFrame(testUsers)\n",
    "    #recosForUser = mod.recommendForUserSubset(testUsers,nItem)\n",
    "    recosForUser = mod.recommendForUserSubset(testUsers,nItem)\n",
    "    \n",
    "    recosForUser = recosForUser.select(recosForUser.user,F.posexplode(recosForUser.recommendations))\n",
    "    recosForUser = recosForUser.withColumn(\"item\",recosForUser[\"col\"].getItem(\"item\"))\n",
    "    recosForUser = recosForUser.drop('col')\n",
    "    testData = testData.rename(columns={'item':'true_item'})\n",
    "    sparkOutput = sqlCtx.createDataFrame(testData[['user','true_item']])\n",
    "    \n",
    "    cond = [recosForUser.user == sparkOutput.user, recosForUser.item == sparkOutput.true_item]\n",
    "    targetPosition = sparkOutput.join(recosForUser,cond,how='left')\n",
    "    MPR_ = targetPosition.select(F.mean('pos')).toPandas().loc[0,'avg(pos)']\n",
    "    MPR_ = 100*(1-MPR_/nItem)\n",
    "    MPR.append(MPR_)\n",
    "    \n",
    "    for K in Ks:\n",
    "        P[K].append(targetPosition.filter(targetPosition['pos'] < K).count()/targetPosition.count()*100)\n",
    "\n",
    "print(\"MPR=\",np.mean(MPR))\n",
    "for K in Ks:\n",
    "    print(\"Precision @\"+str(K)+\"=\",np.mean(P[K]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
