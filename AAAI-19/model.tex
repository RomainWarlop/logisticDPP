\section{Model}
\label{sec:model}
\subsection{Background}
Determinantal Point Processes (DPPs) were originally used  to model a distribution over particles that
exhibit a repulsive effect ~\cite{Vershik2001book}. Recently, interest in leveraging this repulsive
behavior has led to DPPs receiving increasing attention within the machine
learning community.  Mathematically,
discrete DPPs are distributions over discrete sets of points, or in our case
items, where the model assigns a probability to observing a given set of items. Let $\mcI$ denote a
set of items, and $L$ the kernel matrix associated with the DPP whose entries
encode the similarity between items. The probability of observing the set
$\mcI$ is proportional to the determinant of the principal submatrix of $L$
indexed by the items in $\mcI$: $\pp(\mcI) \propto \det L_{\mcI}$ \footnote{To define a probability measure on the DPP only, the scaling factor is $\det(L + I)$ because $\sum_{\mcI} \det L_{\mcI} = \det (L+I)$.}.
Thus, if $p$ denotes the number of items in the item catalog, the DPP is a probability measure on
$2^p$ (the power set, or set of all subsets of $p$). The kernel $L$ encodes the
similarities between items, where $L_{ij}$ represents the similarity between item
$i$ and item $j$.
A determinant can be seen as a volume from a geometric viewpoint, and therefore more diverse sets 
will tend to have larger determinants.
For example \footnote{$K$ is marginal kernel of $L$ with $K=L(L+I)^{-1}$ see Theorem 2.2 of \cite{Kulesza:2012:DPP:2481023}. We use here and only here the marginal kernel because it has a nice formula for size $2$ subset for interpretation of the repulsion behaviour of DPPs.}, the probability of selecting two items $i$ and $j$ together can be computed as
\begin{eqnarray}
\label{dpp_intuition}
\pp[\{i,j\}] = \begin{vmatrix}
K_{ii} & K_{ji} \\ K_{ji} & K_{jj} 
\end{vmatrix} = \pp[\{i\} \in X] \pp[\{j\} \in X]-K_{ij}^2
\end{eqnarray}
In equation~\ref{dpp_intuition} we can see that the more similar $i$ and $j$
are, the less likely they are to be sampled together. The definition of the
entries $K_{ij}$ will therefore determine the repulsive behavior of the kernel
for the task. For instance, if similarity is defined using image descriptors,
then images of differing appearance will be selected by a DPP. On the other
hand, if the entries $K_{ij}$ are learned using previously observed sets, such as
e-commerce baskets~\cite{DBLP:conf/aaai/GartrellPK17}, then co-purchased items $i$ and
$j$ will are likely to be sampled by the DPP, and thus the "similarity" $K_{ij}$
will be low. In an application such as a search engine or in document summarization, the
kernel may be defined using feature descriptors $\psi_i \in \rr^D$ (i.e tf-idf
of the text), and a relevance score $q_i \in \rr^+$ of each item $i$ such that
$L_{ij} = q_i \psi_i^T \psi_j q_j$, which favors relevant items (large $q_i$) and
discourages lists composed of similar items. 

\subsection{Logistic DPP}
Our objective is to find a set of items that are most likely be purchased
together. We formulate this as a classification problem, where the goal is to
predict if a specific set of items will generate a conversion from the user,
which we denote as $Y \in \{0,1\}$. We model the class label $Y$ as a Bernoulli random
variable of parameter $\phi(\mcI)$, where $\mcI$ is the set of items and $\phi$
is a function that we will define later:
\begin{equation}
p(y|\mcI) = \phi(\mcI)^y(1-\phi(\mcI))^{1-y}
\end{equation} 
We model the function $\phi$ using a DPP. 

We assume that there exists a latent space such that diverse items in this space
are likely to be purchased together. Similarly
to~\cite{DBLP:conf/aaai/GartrellPK17}, we introduce a low-rank factorization of
the kernel matrix $L \in \rr^{p \times p}$:
\begin{equation}
L = VV^T+D^2
\end{equation}
where $V \in \rr^{p \times r}$ is a latent matrix where each row vector $i$
encodes the $r$ latent factors of item $i$.  $D$ is a diagonal matrix that, and
together with $||V_i||$, represents the intrinsic quality or popularity of each
item. The squared exponent on $D$ insures that we always have a valid positive
semi-definite kernel. We then define $\phi(\mcI) \propto \det (V_{\mcI,:}
V_{\mcI,:}^T+D^2) \geq 0$. Note that without the diagonal term, the choice of
$r$ would restrict the cardinality of the observable set, because $|\mcI| > r$
would imply $\phi(\mcI) = 0$ when $D\equiv0$. Using this term will ensure that
the success probability of any set will be positive, but the cross-effects will
be lower for sets of cardinality higher than $r$. We also see that items with
similar latent vectors are less likely to be sampled than items with different
latent vectors, since similar vectors will produce a parallelotope with a
smaller volume. To normalize the probability and encourage separation between
vectors we use a logistic function on $\Phi$ such that:
\begin{eqnarray}
\label{themodel}
\phi(I) = \pp(y=1|\mcI) & \doteq & 1-\exp(-w \det L_{\mcI}) \\
& \doteq & \sigma(w \det L_{\mcI})
\end{eqnarray}
Usually the logistic function is of the form $1/(1+\exp(-w \det L_{\mcI}))$.
However, in our case the determinant is always positive since $L$ is positive
semi-definite, which would results in $\pp(y=1|\mcI)$ always greater than $0.5$ which such a function. By
construction, our formulation allows us to obtain a probability between $0$ and
$1$. Finally, $w \in \rr$ is a scaling parameter that insures that the exponential
does not explode, since the diagonal parameter will be approximately $1$. \\
%
\textbf{Learning}. In order to learn the matrix $V$ we assume the existence of
historical data $\{\mcI_m,y_m\}_{1 \leq m \leq M}$, where $\mcI_m$ is a set of
items, and $y_m$ is a label set to $1$ if the set has been purchased and $0$
otherwise. This training data allows us to learn the matrices $V$ and $D$ by maximizing the
log-likelihood of the data. To do so, we first write the click probability for
all $y$ as
\begin{equation}
\label{themodel_y}
\pp(y|\mcI) = \sigma(w \det L_{\mcI})^y (1-\sigma(w \det L_{\mcI}))^{1-y} 
\end{equation}
The log-likelihood $f(V)$ can then be written as
\begin{eqnarray}
\label{loglikelihood_ST}
\small
f(V,D) & = & \log \prod_{m=1}^M \pp(y_m|\mcI_m)  \\ 
& & \quad - \frac{\alpha_0}{2} \sum_{i=1}^p \alpha_i (||V_i||^2 + ||D_i||^2) \nonumber \\
& = & \sum_{m=1}^M \log \pp(y_m|\mcI_m) \nonumber \\
& & \quad - \frac{\alpha_0}{2} \sum_{i=1}^p \alpha_i (||V_i||^2 + ||D_i||^2)
%& = & \sum_{m=1}^M  y_m \log \sigma(\det L_{\mcI_m}) \\
%& & + (1-y_m) \log (1-\sigma(\det L_{\mcI_m})) - \frac{\alpha_0}{2} \sum_{i=1}^p \alpha_i (||V_i||^2 + ||D_i||^2) \nonumber
\end{eqnarray}
Following \cite{DBLP:conf/aaai/GartrellPK17}, $\alpha_i$ is an item regularization weight that is inversely proportional to item popularity. \\
%
\textbf{Optimization}. We maximize the log-likelihood using stochastic gradient
ascent with Nesterov's Accelerated Gradient, which is a form of momemtum. To
simplify notation, we define $[m] \doteq \mcI_m$ and $\sigma_m = \sigma(w \det
L_{[m]})$. Let $i \in \{1, \cdots, p \}, k \in \{1, \cdots, r\}$, 
%%%%%%%%%%%%%%%
\noindent
\textbf{lemma} When $D$ is fixed, the gradient of (\ref{loglikelihood_ST}) with respect to $V_{ik}$ is
\begin{eqnarray}
\label{ST_Vgradient}
\frac{\partial f}{\partial V_{ik}} & = & 2 w \sum_{m, i \in [m]} ([L_{[m]}^{-1}]_{:,i} V_{:,k}) \frac{y_m-\sigma_m}{\sigma_m} \det L_{\mcI_m} \\ 
& & - \alpha_0 \alpha_i V_{ik} \nonumber
\end{eqnarray}

% \textbf{Proof} Without the regularization term we have
% {\small
% \begin{eqnarray}
% \frac{\partial f}{\partial V_{ik}} & = & \sum_{m, i \in [m]} \frac{y_m}{\sigma_m} \frac{\partial \sigma_m}{\partial V_{ik}}+ \frac{1-y_m}{1-\sigma_m}\left(-\frac{\partial \sigma_m}{\partial V_{ik}} \right) \\
% & = & w \sum_{m, i \in [m]} \frac{y_m-\sigma_m}{\sigma_m} \frac{\partial \det L_{[m]}}{\partial V_{ik}} \\
% & = & w \sum_{m, i \in [m]} \frac{y_m-\sigma_m}{\sigma_m} \ \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial V_{ik}}\right) \det L_{\mcI_m} \\
% & = & 2 w \sum_{m, i \in [m]} ([L_{[m]}^{-1}]_{:,i} V_{:,k}) \frac{y_m-\sigma_m}{\sigma_m} \det L_{\mcI_m} \label{gradient_last_step}
% \end{eqnarray}}
% where (\ref{gradient_last_step}) follows from
% \begin{equation}
% \left[\frac{\partial L_{[m]}}{\partial V_{ik}}\right]_{s,t} = V_{sk} \delta_{i,t} + V_{tk} \delta_{i,s}
% \end{equation}
% Therefore,
% \begin{eqnarray}
% \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial V_{ik}}\right) & = & \sum_{s,t} ( V_{sk} \delta_{i,t} + V_{tk} \delta_{i,s}) [L_{[m]}^{-1}]_{s,t} \\
% & = & \sum_s [L_{[m]}^{-1}]_{s,i} V_{sk} + \sum_t [L_{[m]}^{-1}]_{i,t} V_{tk} \nonumber \\
% & = & 2 \sum_s [L_{[m]}^{-1}]_{s,i} V_{sk} %\quad \text{ (since } L_{[m]}^{-1} \text{ is symmetric)}
% \end{eqnarray}
% adding the derivative of the regularization concludes the proof. $\square$

%%%%%%%%%%%%%%%
\noindent
\textbf{lemma} When $V$ is fixed, the gradient of (\ref{loglikelihood_ST}) with respect to $D_i$ is
\begin{eqnarray}
\label{ST_Dgradient}
\frac{\partial f}{\partial D_{ii}} & = & 2 w \sum_{m, i \in [m]} ([L_{[m]}^{-1}]_{i,i} D_{i,i}) \frac{y_m-\sigma_m}{\sigma_m} \det L_{[m]} \\
& & - \alpha_0 \alpha_i D_{ii} \nonumber
\end{eqnarray}

% \textbf{Proof} As shown prevously and without the regularization term we have
% {\small
% \begin{eqnarray}
% \frac{\partial f}{\partial V_{ik}} = w \sum_{m, i \in [m]} \frac{y_m-\sigma_m}{\sigma_m} \ \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial D_{i,i}}\right) \det L_{\mcI_m} 
% \end{eqnarray}}
% Since,
% \begin{equation}
% \left[\frac{\partial L_{[m]}}{\partial D_{i,i}}\right]_{s,t} = 2 D_{i,i} \delta_{s,i} \delta_{t,i}
% \end{equation}
% we see that
% {\small
% \begin{eqnarray}
% \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial D_{i,i}}\right) & = & 2 [L^{-1}_{[m]}]_{i,i} D_{i,i}
% \end{eqnarray}}
% adding the  derivative of the regularization concludes the proof. $\square$

\begin{algorithm}[t]
\begin{algorithmic}
%\STATE \textbf{Notation:} $c(t,a) = R \sqrt{(d+1) \log\left(L t^{\alpha} \left(1+\frac{t_a L_w^2}{\lambda}\right) \right)}+\lambda^{1/2}B$
\STATE \textbf{Input:} $\alpha_0 \in \rr$, $\beta \in \rr$ the momentum coefficient, $m \in \nn$ the minibatch size, $\varepsilon \in \rr$ the gradient step, $t=0$ the iteration counter, $T=0$, past data $\mcD = \{\mcI_m,y_m\}_{1 \leq m \leq M}$.
\STATE \textbf{Initialization:} Compute item popularity and output regularization weights $\alpha_i$. 
\STATE Set $D_0 \sim \mathcal{N}(1,0.01)$ on the diagonal and $\tilde{D}_0 \equiv 0$ the gradient accumulation on $D$.
\STATE Set $V_0 \sim \mathcal{N}(0,0.01)$ everywhere and $\tilde{V}_0 \equiv 0$ the gradient accumulation on $V$.
\WHILE{not converged}
\IF{$m (t+1) > M (T+1)$}
\STATE Shuffle $\mcD$ and set $T = T +1 $
\ENDIF 
\STATE Update $(\tilde{V}_{t+1}, \tilde{D}_{t+1}) = \beta (\tilde{V}_t,\tilde{D}_t) + (1-\beta) \varepsilon \bigtriangledown f(V_t+\beta \tilde{V}_t, D_t+\beta \tilde{D}_t)$ according to formulas (\ref{ST_Vgradient}) and (\ref{ST_Dgradient})
\STATE Update $V_{t+1} = V_t + \tilde{V}_{t+1}$
\STATE Update $D_{t+1} = D_t + \tilde{D}_{t+1}$ 
\ENDWHILE
\end{algorithmic}
\caption{Optimization algorithm for Logistic DPP model.}
\label{alg:singletask}
\end{algorithm}

\subsection{Multi-task DPP}
We now propose a modification to the previously introduced model that is better
suited for the basket completion task. 
To do so we enhance the logistic DPP for the basket completion scenario, where we model the probability that the user will purchase a
given additional item based on the items already present in the user's shopping basket.
We formulate this as a multi-task classification problem, where the goal is
to predict whether the user will purchase a given target item based on the
user's basket. In this setting there are as many tasks as there are items in the
catalog, $p$ (minus the items already in the basket). Learning one kernel per
task would be impossible in practice and suffer from sparsity issues. Indeed,
with one kernel per task, each target item would be present in only a fraction
of the baskets, and thus dramatically reduce the size of the training set per
kernel. To solve this issue we utilize a low-rank tensor. We use a cubic tensor
$L \in \rr^{p \times p \times p}$, where each slice $\tau$ (noted $L_{\tau}$) of
$L$ is the task (low-rank) kernel. By assuming that the tensor $L$ is low-rank,
we are able to implement sharing of learned parameters between tasks, as shown in
the following equation:
\begin{equation}
\label{rescal_decomposition}
L_{\tau} = V R_{\tau}^2 V^T + D^2
\end{equation}
where $V \in \rr^{p \times r}$ are the item latent factors that are common to all
tasks, and $R_{\tau} \in \rr^{r \times r}$ is a task specific matrix that models
the interactions between the latent components of each task. In order to balance
the degrees of freedom between tasks and items, we further assume that $R_{\tau}$ is
a diagonal matrix. Therefore, the diagonal vector of $R_{\tau}$ models the latent
factors of each task, and the latent factors of the item can be seen as the
relevance of the product for each latent factor. As is the case for the matrix $D$, the
squared exponent on $R_{\tau}$ ensures that we always have a valid kernel.
This decomposition is similar to the RESCAL (\cite{ICML2011Nickel_438}) decomposition without the additional bias term and a diagonal constraint on the slice specific matrix. 
Besides we also have a different learning procedure due to the use of the logistic function.  
Finally, the probability that a set of items $\mcI$ will be successful for task
$\tau$ is
\begin{equation}
\label{themodel_multitask}
\pp(y_{\tau}=1|\mcI) = \sigma(w \det L_{\tau,\mcI})= 1-\exp(-w \det L_{\tau,\mcI}) 
\end{equation}
%
Therefore, the log-likelihood is
\begin{eqnarray}
\label{loglikelihood_MT}
g(V,D,R) & = & \log \prod_{m}^{M} \pp(y_{\tau}|[m]) \\
& & \quad - \frac{\alpha_0}{2} \sum_{i=1}^p \alpha_i (||V_i||^2 + ||D_i||^2 + ||R^i||^2) \nonumber \\
& = & \sum_{m}^{M} \log \pp(y_{\tau}|[m]) \\
& & \quad - \frac{\alpha_0}{2} \sum_{i=1}^p \alpha_i (||V_i||^2 + ||D_i||^2 + ||R^i||^2) \nonumber
\end{eqnarray}

\noindent
Since each observation $m$ is attached to a task, we denote $\tau_m$ as the task
that corresponds to observation $m$. Thus we have $\sigma_{m} = \sigma(\det
L_{\tau_m,[m]})$. When there is no ambiguity, we also denote $L_{[m]} \doteq
L_{\tau_m,[m]}$. Let $i \in
\{1, \cdots, p \}, k \in \{1, \cdots, r\}$, 
\\
\noindent
\textbf{lemma} When $D$ and $R$ are fixed, the gradient of (\ref{loglikelihood_MT}) with respect to $V_{ik}$ is
\begin{eqnarray}
\label{MT_Vgradient}
\frac{\partial g}{\partial V_{ik}} & = & 2 w \sum_{m, i \in [m]} \frac{y_{\tau_m}-\sigma_m}{\sigma_m} R_{\tau_m,k,k}^2 [L^{-1}_{\tau_m,[m]}]_{:,i} \nonumber \\
& & \cdot V_{:,k}  \det L_{\tau_m,[m]}  - \alpha_0 \alpha_i V_{ik} 
\end{eqnarray}

% \textbf{Proof} Without the regularization term we have
% {\small
% \begin{eqnarray}
% \frac{\partial g}{\partial V_{ik}} & = & \sum_{m, i \in [m]} \frac{y_{\tau_m}}{\sigma_m} \frac{\partial \sigma_m}{\partial V_{ik}}+ \frac{1-y_{\tau_m}}{1-\sigma_m}\left(-\frac{\partial \sigma_m}{\partial V_{ik}} \right) \\
% & = & w \sum_{m, i \in [m]} \frac{y_{\tau_m}-\sigma_m}{\sigma_m} \frac{\partial \det L_{[m]}}{\partial V_{ik}} \\
% & = & w \sum_{m, i \in [m]} \frac{y_m-\sigma_m}{\sigma_m} \ \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial V_{ik}}\right) \det L_{[m]} \\
% \end{eqnarray}}
% Since,
% \begin{equation}
% \label{kernel_entry}
% [L_{\tau}]_{s,t} - D^2_{s,t} = \sum_{j=1}^r [V R_{\tau}^2]_{s,j} V_{t,j} = \sum_{j=1}^r V_{s,j} R_{\tau,j,j}^2 V_{t,j}
% \end{equation}
% we have
% \begin{equation}
% \left[\frac{\partial L_{[m]}}{\partial V_{ik}}\right]_{s,t} = R_{\tau,k,k}^2 (V_{t,k} \delta_{s,i}+V_{s,k} \delta_{t,i})
% \end{equation}
% Thus, 
% \begin{eqnarray}
% \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial V_{ik}}\right) & = & 2 R_{\tau,k,k}^2 \sum_{s \in [m]} [L^{-1}_{\tau_m,[m]}]_{s,i}  V_{s,k} 
% \end{eqnarray}
% adding the regularization concludes the proof. $\square$

%%%%%%%%%%%%%%%
\noindent
\textbf{lemma} When $V$ and $R$ are fixed, the gradient of (\ref{loglikelihood_MT}) with respect to $D_{i,i}$ is
\begin{eqnarray}
\label{MT_Dgradient}
\frac{\partial g}{\partial D_{i,i}} & = & 2 w \sum_{m, i \in [m]} \frac{y_{\tau_m} - \sigma_m}{\sigma_m} [L^{-1}_{t_m,[m]}]_{i,i} D_{i,i} \det L_{\mcI_m} \nonumber \\
& & - \alpha_0 \alpha_i D_{ii} 
\end{eqnarray}

% \textbf{Proof} Similarly without the regularization term we have
% {\small
% \begin{eqnarray}
% \frac{\partial g}{\partial D_{i,i}} =  w \sum_{m, i \in [m]} \frac{y_m-\sigma_m}{\sigma_m} \ \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial D_{i,i}}\right) \det L_{[m]} 
% \end{eqnarray}}
% Using (\ref{kernel_entry})
% \begin{equation}
% \left[\frac{\partial L_{[m]}}{\partial D_{i,i}}\right]_{s,t} = 2 D_{i,i} \delta_{s,i} \delta_{t,i}
% \end{equation}
% thus
% {\small
% \begin{eqnarray}
% \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial D_{i,i}}\right) & = & 2 [L^{-1}_{t_m,[m]}]_{i,i} D_{i,i}
% \end{eqnarray}}
% adding the regularization concludes the proof. $\square$

%%%%%%%%%%%%%%%
\noindent
\textbf{lemma} When $V$ and $D$ are fixed, the gradient of (\ref{loglikelihood_MT}) with respect to $R_{k,k}$ is
\begin{eqnarray}
\label{MT_Rgradient}
\frac{\partial g}{\partial R_{\tau,k,k}} & = & 2 w \sum_{m, \tau \in [m]} \frac{y_{\tau} - \sigma_m}{\sigma_m} \ R_{\tau,k,k} {L^{-1}}_{[m]} \cdot V_{:,k} \nonumber\\
& & \cdot V_{:,k}^T  \det L_{[m]} - \alpha_0 \alpha_{\tau} R_{\tau,k,k,} 
\end{eqnarray}

% \textbf{Proof} Similarly without the regularization we have
% {\small
% \begin{eqnarray}
% \frac{\partial g}{\partial R_{\tau,k,k}} = w \sum_{m, i \in [m]} \frac{y_{\tau}-\sigma_m}{\sigma_m} \ \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial R_{\tau,k,k}}\right) \det L_{[m]} \\
% \end{eqnarray}}
% Using (\ref{kernel_entry})
% \begin{equation}
% \left[\frac{\partial L_{[m]}}{\partial R_{\tau,k,k}}\right]_{s,t} = 2 R_{\tau,k,k} V_{s,k} V_{t,k}
% \end{equation}
% we obtain
% {\small
% \begin{eqnarray}
% \text{tr}\left(L_{[m]}^{-1} \frac{\partial L_{[m]}}{\partial R_{\tau,k,k}}\right) & = & 2 R_{\tau,k,k} \sum_{s,t \in [m]} [L^{-1}_{[m]}]_{s,t} V_{s,k} V_{t,k}
% \end{eqnarray}}
% adding the regularization concludes the proof. $\square$

\begin{algorithm}[t]
\begin{algorithmic}
%\STATE \textbf{Notation:} $c(t,a) = R \sqrt{(d+1) \log\left(L t^{\alpha} \left(1+\frac{t_a L_w^2}{\lambda}\right) \right)}+\lambda^{1/2}B$
\STATE \textbf{Input:} $\alpha_0 \in \rr$, $\beta \in \rr$ the momentum coefficient, $m \in \nn$ the minibatch size, $\varepsilon \in \rr$ the gradient step, $t=0$ the iteration counter, $T=0$, past data $\mcD = \{\mcI_m,y_m\}_{1 \leq m \leq M}$.
\STATE \textbf{Initialization:} Compute item popularity and output regularization weights $\alpha_i$. 
\STATE Set $D_0 \sim \mathcal{N}(1,0.01)$ on the diagonal and $\tilde{D}_0 \equiv 0$ the gradient accumulation on $D$.
\STATE Set $V_0 \sim \mathcal{N}(0,0.01)$ everywhere and $\tilde{V}_0 \equiv 0$ the gradient accumulation on $V$.
\STATE Set $R_{\tau,0} \sim \mathcal{N}(1,0.01)$ on the diagonal for each task and $\tilde{R}_{\tau,0} \equiv 0$ the gradient accumulation on $R_{\tau}$.
\WHILE{not converged}
\IF{$m (t+1) > M (T+1)$}
\STATE Shuffle $\mcD$ and set $T = T +1 $
\ENDIF 
\STATE Update $\left(\tilde{V}_{t+1}, \tilde{D}_{t+1}, (\tilde{R}_{\tau,t+1})_{\tau}\right) = \beta \left(\tilde{V}_t,\tilde{D}_t,(\tilde{R}_{\tau,t})_{\tau} \right) + (1-\beta) \varepsilon \bigtriangledown g(V_t+\beta \tilde{V}_t, D_t+\beta \tilde{D}_t, (R_{\tau,t}+\beta \tilde{R}_{\tau,t})_{\tau})$ according to formulas (\ref{MT_Vgradient}), (\ref{MT_Dgradient}) and (\ref{MT_Rgradient})
\STATE Update $V_{t+1} = V_t + \tilde{V}_{t+1}$
\STATE Update $D_{t+1} = D_t + \tilde{D}_{t+1}$ 
\STATE Update $R_{\tau,t+1} = R_{\tau,t} + \tilde{R}_{\tau,t+1}$ for all $\tau$ 
\ENDWHILE
\end{algorithmic}
\caption{Optimization algorithm for Multi Task DPP model.}
\label{alg:multitask}
\end{algorithm}

\subsection{Prediction}
As discussed previously sampling from a DPP can be a difficult problem, and
various solution have been
proposed~\cite{DBLP:journals/corr/HanKPS17,NIPS2014_5564,DBLP:conf/icml/GautierBV17,DBLP:journals/corr/abs-1709-05135}.
Although sampling the best set among all possible set has been conjectured to be
NP-hard, our goal is to find only the best item to complete the basket. In such
applications a greedy approach can be applied effectively, especially with the
low-rank structure of our model. In addition,~\cite{DBLP:conf/aaai/GartrellPK17}
proposed an effective method for the basket completion scenario that involves
conditioning the DPP, which can be applied to our Logistic DPP model. 